{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loading TFDS in TPU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/ba+FVzA+HViFMr63qNIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgc-aiml/DC-GAN-using-TPU/blob/main/Loading_TFDS_in_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HeyhyP0nVrli"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import os\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define strategy"
      ],
      "metadata": {
        "id": "_dryiHNlftRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  strategy == None\n",
        "except:\n",
        "  # strategy = tf.distribute.MirroredStrategy()\n",
        "# print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  # This is the TPU initialization code that has to be at the beginning.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "  # optimizer = tf.tpu.CrossShardOptimizer(tf.train.GradientDescentOptimizer(0.01))\n",
        "\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW-o9vu2ftbG",
        "outputId": "30547118-eeb9-4949-f827-55eb156c75d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load tfds"
      ],
      "metadata": {
        "id": "DFfaLQPGWg7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = tfds.load('mnist', split='train', as_supervised=True )\n",
        "test_ds = tfds.load('mnist',split='test', as_supervised=True )\n"
      ],
      "metadata": {
        "id": "W8UPlzeRV5id"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tfds for TPU"
      ],
      "metadata": {
        "id": "7jS4Ud8FwvFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = tfds.load('mnist', split='train', as_supervised=True,try_gcs=True)\n",
        "test_ds = tfds.load('mnist',split='test', as_supervised=True,try_gcs=True)\n"
      ],
      "metadata": {
        "id": "g540F0NFwvTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare the data"
      ],
      "metadata": {
        "id": "_0DlNeMvZSX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_numpy = tfds.as_numpy(train_ds)\n",
        "test_numpy = tfds.as_numpy(test_ds)\n",
        "\n",
        "train_images = np.zeros((len(train_ds),28,28,1),dtype=np.uint8)\n",
        "train_label = np.zeros((len(train_ds),))\n",
        "\n",
        "test_images = np.zeros((len(test_ds),28,28,1),dtype=np.uint8)\n",
        "test_label = np.zeros((len(test_ds),))\n",
        "\n",
        "\n",
        "pos = 0\n",
        "for image,label in train_numpy:\n",
        "  train_images[pos,:,:,:] = image\n",
        "  train_label[pos] = label\n",
        "  pos = pos + 1\n",
        "\n",
        "pos = 0\n",
        "for image,label in test_numpy:\n",
        "  test_images[pos,:,:,:] = image\n",
        "  test_label[pos] = label\n",
        "  pos = pos + 1\n",
        "  "
      ],
      "metadata": {
        "id": "tHb4oSXeZUQE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the model"
      ],
      "metadata": {
        "id": "HpxJ7b-EWm6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_classifier(fig_shape=(28,28,1)):\n",
        "  cinputA = tf.keras.Input(shape=fig_shape)\n",
        "  clayer = layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\")(cinputA)\n",
        "  clayer = layers.LeakyReLU(alpha=0.2)(clayer)\n",
        "\n",
        "  clayer = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(clayer)\n",
        "  clayer = layers.LeakyReLU(alpha=0.2)(clayer)\n",
        "\n",
        "  # dlayer = layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(dlayer)\n",
        "  # dlayer = layers.LeakyReLU(alpha=0.2)(dlayer)\n",
        "  clayer = layers.Flatten()(clayer)\n",
        "  clayer = layers.Dropout(0.2)(clayer)\n",
        "\n",
        "  clayer = layers.Dense(500)(clayer)\n",
        "  clayer = layers.LeakyReLU(alpha=0.2)(clayer)\n",
        "  clayer = layers.Dropout(0.2)(clayer)\n",
        "\n",
        "  clayer2 = layers.Dense(10, activation=\"softmax\")(clayer)\n",
        "\n",
        "  classifier = tf.keras.models.Model(inputs=cinputA, outputs=clayer2)\n",
        "  return classifier"
      ],
      "metadata": {
        "id": "KwEHTF_KWU0w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model without TPU"
      ],
      "metadata": {
        "id": "X1z9cAdBWslP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(train_ds)\n",
        "BATCH_SIZE = 128\n",
        "fig_shape = (28,28,1)\n",
        "classifier_final = make_classifier(fig_shape)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False)\n",
        "classifier_final.compile( optimizer=\"adam\", loss=loss_object,metrics=['Accuracy'])\n",
        "train_ds_batched = train_ds.batch(BATCH_SIZE)\n",
        "classifier_final.fit(train_ds_batched,epochs=5)"
      ],
      "metadata": {
        "id": "BC8JDFZfZPjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train in TPU using tuple"
      ],
      "metadata": {
        "id": "XG3C42QifznL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  # BUFFER_SIZE = len(train_ds)\n",
        "  \n",
        "  fig_shape = (28,28,1)\n",
        "  classifier_final = make_classifier(fig_shape)\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=False)\n",
        "  classifier_final.compile( optimizer=\"adam\", loss=loss_object,metrics=['Accuracy'])\n",
        "  train_ds_batched = train_ds.batch(BATCH_SIZE)\n",
        "  classifier_final.fit(train_ds_batched,epochs=5)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6J3H7LgfzLl",
        "outputId": "a0d222d1-d6c1-4117-b352-c3338090d5ea"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 12s 17ms/step - loss: 0.5384 - Accuracy: 0.9259\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1029 - Accuracy: 0.9700\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0808 - Accuracy: 0.9759\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0754 - Accuracy: 0.9780\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0758 - Accuracy: 0.9786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train in TPU using numpy array"
      ],
      "metadata": {
        "id": "GqmPintef3Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(train_ds)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "with strategy.scope():\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((train_images,train_label)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "  train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "  \n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((test_images,test_label)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "  test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n",
        "\n",
        "with strategy.scope():\n",
        "  \n",
        "  BATCH_SIZE = 128\n",
        "  fig_shape = (28,28,1)\n",
        "  classifier_final = make_classifier(fig_shape)\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=False,\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "  classifier_final.compile( optimizer=\"adam\", loss=loss_object,metrics=['Accuracy'])\n",
        "\n",
        "  classifier_final.fit(train_dataset,epochs=5)\n",
        "  classifier_final.evaluate(test_dataset)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGpuiYdPoV8D",
        "outputId": "a808c9df-3473-4e89-aa73-c1ff6a7ffaed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 15s 22ms/step - loss: 0.6594 - Accuracy: 0.9176\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.1080 - Accuracy: 0.9686\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0833 - Accuracy: 0.9758\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0756 - Accuracy: 0.9787\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0771 - Accuracy: 0.9785\n",
            "79/79 [==============================] - 5s 29ms/step - loss: 0.0986 - Accuracy: 0.9747\n"
          ]
        }
      ]
    }
  ]
}